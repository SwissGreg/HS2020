 \documentclass[8pt]{extreport}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{geometry}
\usepackage{float}
\usepackage{fixltx2e}
\geometry{a4paper, margin=1in}
\title{Numerical Methods\\Summary}
\begin{document}
	\maketitle
	\newpage


\chapter{Computing with Matrices and Vectors}

\textbf{\underline{Elementary Operations:}} +,-,*,$\backslash,$ lowest level of real arithmetic available on computers usually implemented in hardware\\
\textbf{\underline{Elementary Linear Algebra operations:}} The next level real arithmetic which is the computation on finite arrays of real numbers\\
\textbf{\underline{Complex Algorithms:}} involves iterations and approximations 

\section{Fundamentals}

$\mathbb{K}$ notation for a generic field of numbers i.e from $\mathbb{R}$ or $\mathbb{C}$

\underline{\textbf{Vectors:}} one-dimensional array of real/complex numbers, the default for this lecture are column vectors:

\begin{figure}[H]
\centering
\includegraphics[width = 50mm]{Num1.png}
\end{figure}
\textbf{Unless stated otherwise, in mathematical formulas vector components are indexed from 1}

\textbf{Notations:}
\begin{itemize}
\item column vectors: bold small roman letters e.g \textbf{x,y,z}
\item row vectors: \textbf{$x^T,y^T,z^T$}
\item Addressing vector components:
\begin{center}
$x =[x_1,...,x_n]^T \rightarrow x_i, i = 1,...,n$\\
$x \in \mathbb{K}^n \rightarrow (x)_i, i=1,...,n$
\end{center}
\item Selecting sub-vectors: $(x)_{k:l} = [x_k,...,x_l]^T,  1 \leq k \leq l \leq n$
\item j-th unit vector: $(e_j)_i = \sigma_{ij}$ where $sigma_{ij}:=1 \text{ if } i = j$ 0 else (Kronecker symbol)
\end{itemize}

\section{Software and Libraries}

\paragraph{Eigen:} A header-only C++ template library designed to enable easy, natural and efficient numerical linear algebra

\paragraph{Header-Only} A library is called header-only if the full definitions of all macros functions and classes comprising the library are visible to the compiler in a header file form. Header-only libraries do not need to be separately compiled, packaged and installed in order to be used. All that is required is to point the compiler at the location of the headers, and then include the header files into the application source

\paragraph{Header File form:} Many programming languages and other computer files have a directive, often called inlcude (or copy, import) that causes the contents of a second file to be inserted into the original file. These included files are called copybooks or header files.


\textbf{\underline{Eigen Cheet Sheet:}}
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.99\linewidth}
\includegraphics[width = \linewidth,scale = 1]{Num2.png}
\end{subfigure}
\begin{subfigure}[b]{0.99\linewidth}
\includegraphics[width = \linewidth,scale = 1]{Num3.png}
\end{subfigure}
\begin{subfigure}[b]{0.99\linewidth}
\includegraphics[width = \linewidth,scale = 1]{Num4.png}
\end{subfigure}
\end{figure}

The Fundamental type of Eigen is the Matrix. There are two types of matrices:
\begin{itemize}
\item \textbf{fixed size:} size known at compile time
\item \textbf{dynamic:} size known only at run time
\end{itemize}

\paragraph{Tensor product} A column vector multiplied with a row vector
\paragraph{Dot Product} Row vector multiplied with a column vector

\section{1.2.3 Matrix Storage Formats}

The entries of a (generic, dense i.e every entry matters) $A \in \mathbb{K}^{m,n}$ are stored in a contiguous linear array of size $m \cdot n$. An exception is structured/sparse matrices i.e matrices which have a certain structure or few non zero entries. \\

By Default Eigen stores matrices in Column major format as all the elements in this format are contiguous in memory. In Row major format the elements of the Matrix are scattered which results in cache misses. Hence Column major format is more efficient  

\underline{Accessing Arrays in Eigen:} 
\begin{itemize}
\item A(i) $\rightarrow$  reference to the i-th element of the array
\item A.data() $\rightarrow$ raw pointer
\end{itemize} 

We declare a nxn matrix in Eigen like:

\begin{center}
Eigen:: MatrixXd A = Eigen:: MatrixXd::Random(n,n)
\end{center}

Row/Col Access (j-th row):
\begin{center}
A.row(j)\\
A.col(j)
\end{center}


\subsection{Raw Pointers}

A pointer is a type of variable. It stores the address of an object in memory and is used to access that object in memory and is used to access that object.\\
A \textbf{Raw Pointer} is a pointer whose lifetime is not controlled by an encapsulating object . A raw pointer can be assigned the address of another non-pointer variable, or it can be assigned a value of nullptr.
\begin{figure}[H]
\centering
\includegraphics[width = 70mm]{Num5.png}
\end{figure}


\section{1.4 Computational Effort} 

Traditional: number of elementary operations\\

Modern: The computational effort involved in a run of a numerical code is only loosely related to the overall execution time on modern computers. It is mainly determined by the memory access pattern and vectorization/pipelining.

\subsection{1.4.1 Asymptotic complexity}

Characterises the worst-case dependence of its computational effort on one or more problem size parameters when these tend to $\infty$\\

\underline{Notation:} Landau-O notation\\

We define the computational effort as Cost(n) = $\mathcal(O)(n^{\alpha}), \alpha > 0 \iff \exists C >0, n_0 \in \mathbb{N}: cost(n)\leq Cn^{\alpha} \forall n > n_0$\\

\underline{Tacit Sharpness assumption:} $cost(n) \neq \mathbb{O}(n^{\beta}), \forall \beta < \alpha$\\

Asymptotic complexity predicts the dependence of runtime on problem size for large problems, because for small problem sizes we can use the caches and hence we do not have the memory access bottleneck. 

\subsection{1.4.2 Computational Cost of basic numerical LA operations}
\begin{figure}[H]
\centering
\includegraphics[width = 70mm]{Num6.png}
\end{figure}

The matrix multiplication is implemented with a triple loop and hence not optimal.\\

A doubly logarithmic plot is scaled logarithmically on the x and y axis. If Cost(n) = $ \mathbb{N}(n^{\alpha})$ then the data points aligned in a doubly logarithmic plot correspond to a straight line.  

\subsection{1.4.3 Tricks to improve complexity}
\begin{itemize}
\item \underline{\textbf{Exploit associativity}} 
\begin{figure}[H]
\centering
\includegraphics[width = 70mm]{Num7.png}
\end{figure}
\item \underline{\textbf{Hidden Summation:}}
\begin{figure}[H]
\centering
\includegraphics[width = 70mm]{Num8.png}
\end{figure}
\item \underline{\textbf{Reuse of intermediate results}} 
\begin{figure}[H]
\centering
\includegraphics[width = 70mm]{Num9.png}
\end{figure}

\end{itemize}

\section{1.5 Machine Arithmetic and Consequences}
 
Computers can only calculate with finite numbers and hence cannot calculate with real numbers. Computers compute with machine numbers denoted: $\mathbb{M}$ The set M has two properties:
\begin{itemize}
\item \underline{\textbf{finite}} I.e there is a lowest and a highest machine number any numbers larger or smaller than these bounds result in overflow/underflow
\item \underline{\textbf{discrete in $\mathbb{R}$}} hence there are gaps in the Machine numbers. When working in $\mathbb{R}$ our computer must round the numbers such that it can work with them. 
\end{itemize}

Roundoff will effect any result of a numerical calculation. Hence a result which should be 0 might not be zero. Therefore we cannot test == 0 we must test for relative smallness

\section{1.5.4 Cancellation}

\underline{\textbf{EPS- Errors:}} Max error you  can have in a Machine set of numbers

EPS-sized errors are troublesome because of error amplification.
\begin{figure}[H]
\centering
\includegraphics[width = 70mm]{Num10.png}
\end{figure}

\underline{\textbf{Cancellation:}} Extreme amplification of relative errors duirng the subtraction of numbers of equal size. 

\subsection{Avoiding Cancellation:} Rewrite expressions inequivalent form immune to cancellation.
For x close to 1 cancellation is harmless.
  
\section{Numerical Stability}

\underline{\textbf{Problem}} A function/mapping $F:X \rightarrow Y$ where X is the data space (input) and Y is the result space (output), in this course we usually consider normed Vector space. On $\mathbb{R}^n$ we have vector norms:
\begin{itemize}
\item $||.||_2$ Euclidean Norm
\item $||.||_1$ One Norm (sum of the absolute value)
\item $||.||_\infty$ Maximum Norm (max value)
\end{itemize}
The vector norms induce \textbf{Matrix Norms}.
\underline{\textbf{D1.5.5.10 Matrix Norm:}} Given vector norms $||.||_x$ and $||.||_y$ on $\mathbb{K}^n \text{ and } \mathbb{K}^m$, respectively, the associated matrix norm is defined by 
\begin{center}
$M \in \mathbb{R}^{m,n}: ||M||:= \underset{x \in \mathbb{R}^n\backslash\{0\}}{sup} \frac{||Mx||_y}{||x||_x}$
\end{center}

Norms are used to understand the size of pertubations.

\underline{\textbf{Stable Algorithm:}} An algorithm $\tilde{F}$ for solving a problem $F:X \mapsto Y$ is \textbf{numerically stable} if for all $x \in X$ its result $\tilde{F}(x)$ (possibly affectedby roundoff) is the exact result for "slightly perturbed" data:
\begin{center}
$\exists C \approx 1: \forall x \in X: \exists \tilde{x} \in X: ||x-\tilde{x}||_X \leq Cw(x)EPS||x||_X \wedge \tilde{F}(x) = F(\tilde{x})$
\end{center}
Where:
\begin{itemize}
\item $\tilde{F}$: Algorithm affected by roundoff
\item $\tilde{x}$. perturbed data
\item Cw(x):= number of operations during the execution of the algorithm
\item EPS:= machine precision
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width = 70mm]{Num11.png}
\end{figure}


The impact of roundoff on a stable algorithm is of the same order of magnitude as the effect of the inevitable perturbations due to rounding the input data. When talking about perturbations we mean relative perturbations.

\underline{\textbf{Sensitive Dependence:}} Given a slight perturbation the result varies largely:
\begin{center}
$||F(x) - F(\tilde{x})||$ can be alrge for tiny $||x-\tilde{x}||_x$
\end{center}

 \chapter{Chapter 2: Direct Methods for Linear Systems of Equations}

\section{Intro and Theory of Linear Systems of Equations (LSE's)} 

\underline{\textbf{LSE:}}
\begin{center} 
$A \underline{x} = \underline{b}$
\end{center}
  where:
\begin{itemize}
\item $A \in \mathbb{K}^{n,n}$ a square system/coefficient matrix
\item $\underline{x} \in \mathbb{K}^n$
\item $\underline{b} \in \mathbb{K}^n$

\end{itemize}
A being a square matrix is important, because this ensures that the number of equations is equal to the number of unknowns

\section{ Square LSE}

\subsection{Existence and uniqueness of solutions}

\begin{enumerate}
\item If A is invertible (regular) :
\begin{center}
$ Ax = b \iff \underline{x} = A^{-1}\underline{b}$
\end{center}
\textbf{Do not use matrix inversion to solve LSE with numerical libraries}\\
A matrix $A \in \mathbb{K}^{n,n}$ is regular $\iff$:
\begin{itemize}
\item $detA \neq 0$
\item columns of A are linearly independent
\item rows of A are linearly independent
\item $N(A) = \{z \in \mathbb{K}^{n}: A\underline{z} = \underline{0}\} = \{\underline{0}\}$
\end{itemize}
\end{enumerate}


\section{Theory: Linear Systems of Equations}

\subsection{ Sensitivity/conditioning of LSE}

\underline{\textbf{Sensitivity:}} Quantities how small (relative) perturbation of data lead to changes of the output

\underline{\textbf{Vector Norms $\&$ (induced) Matrix norms $|| . ||$}}
\begin{itemize}
\item $||M|| = \underset{\underline{x} \neq 0}{sup} \frac{||M\underline{X}}{||\underline{x}||} \Rightarrow ||M\underline{x}|| \leq ||M||\cdot||\underline{x}||$
\item $||\underline{x} + \underline{y}|| \leq ||\underline{x}||+ ||\underline{y}||$
\item $||\underline{y} - \underline{y}|| \geq ||\underline{x}|| - ||\underline{y}||$
\end{itemize}

\textbf{Sensitivity strongly depends on the choice of norms}

\underline{\textbf{Lemma 2.2.2.5 Perturbation lemma}}
\begin{center}
$B \in \mathbb{R}^{n,n}, ||B|| < 1 \Rightarrow I + B$ regular $\wedge ||(I+B)^{-1}|| \leq \frac{1}{1-||B||}$
\end{center}

\underline{\textbf{T 2.2.2.4 Conditioning of LSE's}} If A is regular, $\parallel \Delta A \parallel < \parallel A^{-1} \parallel^{-1}$ and (2.2.2.3), then 
\begin{itemize}
\item A + $\Delta$ A is regular/invertible
\item 
\begin{center}
$\frac{\parallel x - \Delta x \parallel}{\parallel x \parallel} \leq \frac{\parallel A^{-1}\parallel \cdot \parallel A \parallel}{1- \parallel A^{-1}\parallel \parallel A \parallel \cdot \frac{\parallel \Delta A \parallel}{\parallel A \parallel}}\cdot \big( \frac{\parallel \Delta b \parallel}{\parallel b \parallel} + \frac{\parallel \Delta A \parallel}{\parallel A \parallel} \big)$
\end{center}
where:
\begin{itemize}
\item $\frac{\parallel x - \Delta x \parallel}{\parallel x \parallel}$ is the relative error of the result 
\item $\big( \frac{\parallel \Delta b \parallel}{\parallel b \parallel} + \frac{\parallel \Delta A \parallel}{\parallel A \parallel} \big)$ are the relative perturbations
\end{itemize}

\end{itemize}

\underline{\textbf{D 2.2.2.7 Condition number of a matrix}}
\begin{center}
$Cond(A) := \parallel A^{-1} \parallel \cdot \parallel A \parallel$
\end{center}
\begin{itemize}
\item \textbf{LSE: Well conditioned} $Cond(A) \approx 1$
\item \textbf{LSE: Ill-conditioned} $Cond(A) >> 1$
\end{itemize}
\begin{figure}[H]
\centering
\includegraphics[width = 70mm]{Num12.png}
\end{figure}

\begin{center}
cond(A)$>>$ 1 $\iff$ columns/rows of A "almost linearly dependent"
\end{center}
\section{2.3-2.5: Gaussian Elimination}

\underline{\textbf{Gauss Elim:}} Successive row manipulation of LSE $A\underline{x} = \underline{b}$ to convert it to triangular form. It consists of 2 steps:
\begin{enumerate}
\item \textbf{forward elimination:} $\mathcal{O}^3$
\item \textbf{back substitution:} $\mathcal{O}^2$
\end{enumerate}  


\underline{\textbf{Block Gauss Elim:}}

\underline{Reminder:} Matrix multiplication is not commutative!

Given the LSE $A\underline{x} = \underline{b}$, where $A \in \mathbb{R}^{n,n}$ we split A into blocks where $A_{1,1} \in \mathbb{R}^{n,n}$ and regular. We can then apply Gauss like for scalars with the exception of commutivity

\underline{\textbf{LU-Decomposition}}

equivalent to GE i.e cost(LU) = $\mathcal{O}(n^3)$

\begin{figure}[H]
\centering
\includegraphics[width = 120mm]{Num13.png}
\end{figure}

We can solve a LSE with LU-decomposition using the following steps:
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num14.png}
\end{figure}

With eigen we can solve LSE with multiple right hand sides i.e we are solving the equation:
\begin{center}
$AX = B$ with $X,B \in \mathbb{R}^{n,l}$
\end{center}

\textbf{Eigen:} X = A.lu().solve(B)\\
Where lu() calculates the LU-decomposition and solve(B) computes the forward and backward substitution for the columns of B
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num15.png}
\end{figure}

\underline{\textbf{Block LU-Decomposition}}
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num16.png}
\end{figure}

\section{Exploiting Structure when solving Linear Systems}

\underline{\textbf{LSE with arrow matrix:}} An arrow matrix is a matrix with the following form:
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num17.png}
\end{figure}
We can solve this LSE with either:
\begin{itemize}
\item LU-decomposition resulting in a runtime of $\mathcal{O}(n^3)$
\item Block GE resulting in a runtime of $\mathcal{O}(n)$
\end{itemize}
The second method is faster but more vulnerable to round-off errors i.e it is more instable

\underline{\textbf{LSE subject to low-rank modification:}} Assume Ax = b is easy to solve, because :
\begin{enumerate}
\item A has a special structure
\item LU-dec is available
\end{enumerate}
Are goal is to solve $\tilde{A}\tilde{x} = b$ where $\tilde{A}$ arises from A by changing a single entry $(A)_{i^*j^*}$\\
\begin{center}
$\tilde{A} = A + uv^T, u,v \in \mathbb{R}^n$
\end{center}
Using a modified Block elimination we get the equation:
\begin{center}
$\tilde{x} = A^{-1}b - \frac{A^{-1}u(v^H(A^{-1}b)) }{1 +  v^H(A^{-1}u)}$

\end{center}
The generalization of this formula is given by:
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num18.png}
\end{figure}

\section{Sparse Linear Systems}

A sparse matrix is characterized by:
\begin{itemize}
\item "almost all" entries = 0
\item for which it is worth while exploiting this fact i.e a 2x2 matrix is not sparse
\end{itemize}
Sparse LSE arise in models of large networks where each node is connected to only a few other nodes.

\subsection{Sparse Matrix storage formats}

A storage format should fulfill the following conditions:
\begin{itemize}
\item Memory ~ nnz(A) (number of non zero elements in A)
\item cost (A$\cdot$ vector) ~ nnz(A)
\item provide easily accessible information about the location of non zero entries
\end{itemize}

\underline{\textbf[{COO/triplet format:}} Stores A as a list/sequence of tuples (i,j,value)

\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num19.png}
\end{figure}

\underline{\textbf{Compresed row storage (CRS)}}
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num20.png}
\end{figure}

\subsection{Sparse Matrices in Eigen}

default format is CRS
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num21.png}
\end{figure}
The challenge is to efficiently initialize the matrix.\\
Setting random entries is inefficient because it causes massive data movement. Setting an element which was 0 before means we have to insert it in val, update $col_ind$ and$row-ptr$ arrays
There are two possibilities to fix this:
\begin{enumerate}
\item Use intermediate COO/triplet format i.e we initialize with triplets and then convert to CRS format
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num22.png}
\end{figure}
\item Use reserve() $\&$ insert(), if nnz per row/column is known ahead of time
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num23.png}
\end{figure}
\end{enumerate}

\subsection{Direct Solution of Sparse LSE's}

Assume: System matrix is in sparse matrix format i.e tells the location of the zero entries. Hence can be exploited by sparse elimination techniques.\\
\underline{Solving a sparse LSE with Eigen}
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num24.png}
\end{figure}
Sparse elimination can fail because:
\begin{itemize}
\item The matrix is not singular
\item The matrix has an awkward structure
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num25.png}
\end{figure}


In practice when using sophisticated sparse matrix solver libraries the cost of solving Ax= b is $\mathcal{O}((nnz(A))^{\alpha})$ where $\alpha \approx 1.5 - 2.5$


\chapter{Direct Methosds for Linear Least Squares Problems}

\underline{\textbf{Overdetermined LSE:}} An LSE with more equations than unknowns i.e $m \geq n$

\section{Overdetermined LiSE's}

\underline{\textbf{Linear parameter estimation in 1D}} We measure 2 quantities connected by a linear physical law
\begin{center}
$y = \alpha x + \beta$ for some unknown parameters $\alpha,\beta \in \mathbb{R}$
\end{center}
We make n measured pairs i.e $(x_i,y_i) \in \mathbb{R}^2, i = 1,...,n, n>2$ in order to determine $\alpha, \beta$\\
if the measurements were precise the following linear equations would be satisfied:
\begin{center}
$y_i = \alpha x_i + \beta \quad \forall i$
\end{center}
this results in and LSE. However the measured values are affected by measurement errors e.g the y's are affected\\
$\Rightarrow$ The RHS might be perturbed \\
$\Rightarrow$ The RHS is no longer a solution for the matrix A
$\Rightarrow$ No solution may exists



\underline{\textbf{Linear regression: linear parameter estimation}} We are now given the following linear law:
\begin{center}
$y = \underline{a}^T\underline{x} + \beta$ with $x,a \in \mathbb{R}^n, \beta \in \mathbb{R} $
\end{center}
Measurements:
\begin{center}
$(\underline{x}_i,y_i) \in \mathbb{R}^n \times \mathbb{R}, i=1,...,m$
\end{center}
Thy system is Overdetermined if $m > n+1$. As in the above example the result might be perturbed and become unsolvable.

\underline{\textbf{Principle in data science:}} You cannot afford not to use every piece of information available

\underline{\textbf{Angles in triangulations}}\\

\underline{Triangulation:} Covering an area in triangles to determine the distance of points.

We have the following pieces of information available:
\begin{itemize}
\item Each angle is supposed to be equal to its measured value
\item The sum of interior angles is $\pi$ for ecery triangle $\rightarrow +\#$triangles extra equations
\item The sum of angles at an interior node is $2\pi \rightarrow + \#$ interior nodes extra equations
\end{itemize}
Hence we have an OD LSE

\section{Least Squares Solution Concepts}

Setting: OD-LSE $A\underline{x} = \underline{b}$

\subsection{Least Squares Solutions: Definition}

A LSQ solution of $A\underline{x} = \underline{b}$ is a vector $x \in \mathbb{R}^n$ that makes the \textbf{residual}:
\begin{center}
$r:= \underline{b} - A\underline{x}$
\end{center}
as small as possible with respect to the euclidean norm.

\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num26.png}
\end{figure}

\underline{Generalization:} $A \in \mathbb{R}^{n,n} regular \Rightarrow Lsq(A,\underline{b}):= \{A^{-1},\underline{x}\}$
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num27.png}
\end{figure}

\underline{\textbf{Geometric interpretation of least squares solution of $A\underline{x} = \underline{b}$}}
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num28.png}
\end{figure}

$A\underline{x}^*$ is closest to $\underline{b}$ in R(A), $x^*:=LSQ$ sol\\
$\Rightarrow A\underline{x}^* = \underline{b}_0$ = orthogonal projection of $\underline{b}$ onto R(A)

Geometric intuition: Since there will always be a point closest to b a least squares solution always exists:
\begin{center}
$ \forall A \in \mathbb{R}^{m,n}, \underline{b} \in \mathbb{R}^m: lsq(A,b) \neq \emptyset$
\end{center}

\subsection{Normal Equations}

The vector $x \in \mathbb{R}^n$ is a least squares solution of the linear system of equations:
\begin{center}
$Ax = b, A \in \mathbb{R}^{m,n}, b \in \mathbb{R}^{m}$
\end{center}
if and only if it solves the \textbf{normal equations (NEQ)}
\begin{center}
$A^TAx = A^Tb$
\end{center}
From this follows:
\begin{center}
$x \in lsq(A,\underline{b}) \iff A^TAx = A^Tb$
\end{center}
Proof is as follows:\\
The residual r is defined as follow: $r:= \underline{b} - A\underline{x}^* \bot R(A) = \{A\underline{y}: \underline{y} \in \mathbb{R}^n\}$\\
$\Rightarrow$ by the definition of Orthogonality we have:
\begin{center}
$(A\underline{y})^T(\underline{b} - A \underline{x}^*) = 0 \quad \forall \underline{y} \in \mathbb{R}^n$
\end{center}
$\Rightarrow A^T(b -A\underline{x}^*) = 0$
\newline

The system matrix of a NEQ is symmetric


\underline{\textbf{Kernel and range of $A^TA$}} For $A \in \mathbb{R}^{m,n}, m \geq n holds:
\begin{center}
$N(A^TA) = N(A)$\\
$R(A^TA) = R(A^T)$
\end{center}

\underline{\textbf{Uniqueness of least squares solutions:}} If $m\geq n$ and $N(A) = \{0\}$ then the linear system of equations
\begin{center}
$Ax = b, A \in \mathbb{R}^{m,n}, b \in \mathb{R}. has a unique last squares solution

\end{center}
\begin{center}
$ x = (A^TA)^{-1}A^Tb$
\end{center}
That can be obtained by solving the normal equations.
Two things follow from the above Corollary:
\begin{itemize}
\item $N(A) = \{0\} \Rightarrow N(A^TA) = \{0\} \Rightarrow A^TA$ is regular
\item $N(A) = \{0\} \Rightarow A^TA is symettric positive definit
\end{itemize}
We know x is a LSQ solution if A fullfills the Full Rank Condition (FRC):
\begin{center}
$N(A) = \{0\} \iff rank(A) = n$
\end{center}
A failure of FRC indicates to:
\begin{itemize}
\item Flawed mathematical model
\item Useless data
\end{itemize}

\subsection{Moore-Penrose Pseudoinverse}

\underline{\textbf{Generalized solution of a linear system of equations:}}
The generalized solution $x^{\dagger} \in \mathbb{R}^n$ of a linear system of equations $Ax = b, \ A \in \mathbb{R}^{m,n}, b \in \mathbb{R}^m$ is defined as:
\begin{center}
$x^{\dagger} := argmin \{||x||_2: x \in lsq(A,b)\}$
\end{center}
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num29.png}
\end{figure}

\underline{\textbf{T3.1.3.6 Formula for generalized solution: }} Given $A \in \mathbb{R}^{m,n}, b \in \mathbb{R}^m$ the generalized solution $x^{\dagger}$ of the linear system of equations $Ax=b$ is given by:
\begin{center}
$x^{\dagger} = V(V^{T}A^{T}AV)^{-1}(V^{T}A^{T}b)$
\end{center} 
where V is any matrix whose columns form a basis of $N(A)^\bot$
The Matrix multiplication $V(V^{T}A^{T}AV)^{-1}(V^{T}A^{T})$ is called the Moore-Penrose pseudoinverse $A^\dagger$ and is independent of the choice of V.

\section{Normal Equation Methods}

\underline{\textbf{Symmetric positive definite matrices: }} $M \in \mathbb{K}^{n,n}$ is symmetric (Hermitian) positive definite (s.p.d) if:
\begin{center}
$M =M ^H$ and $\forall k \in \mathbb{K}^{n}: x^HMx > 0 \iff x \neq 0$
\end{center}
If $x^HMx \geq 0 \quad \forall x \in \mathbb{K}^n$ then M is called \textbf{positive semi-definite}

\underline{\textbf{Algorithm1:}} Normal equation method to solve full-rank least squares problem $Ax=b$
\begin{enumerate}
\item Compute regular matrix $C:= A^TA \in \mathbb{R}^{n,n}$
\item Compute right hand side vector $c:= A^Tb$
\item Solve s.p.d linear system of equations: Cx = C
\end{enumerate}
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num30.png}
\end{figure}

\underline{\textbf{Loss of sparsity:}} Only a concern for large m,n. A matrix A being sparse does not imply that $A^TA$ will also be sparse. e.g
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num31.png}
\end{figure}

We can avoid computing $A^TA$ using the Extended normal equations:
Idea is to use the residual vector $\underline{r}$ as new unknown and split the equations
\begin{center}
$A^TA\underline{x} = A^T\underline{b} \iff A^T(\underline{b} - A \underline{x}) = 0
\end{center}
this gives us the following split:
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num32.png}
\end{figure}

\section{Orthogonal Transformation Methods} 

Like in the GE we want to transform $A\underline{x} = \underline{b} \rightarrow \tilde{A}\underline{x} = \tilde{\underline{b}}$ such that
\begin{enumerate}
\item LSQ solution of $ \tilde{A}\underline{x} = \tilde{\underline{b}}$ is "easy to compute"
\item $lsq(A,\underline{b}) = lsq(\tilde{A},\tilde{\underline{b}})$
\end{enumerate}

Regarding 1: An easy to compute system for GE is a triangular system, we can use the same approach for this purpose
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num33.png}
\end{figure}

Regarding 2: We want a linear transformation which preserves the 2-norm
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num34.png}
\end{figure}

\subsection{Orthogonal Matrices}
\begin{itemize}
\item $Q \in \mathbb{K}^{n,n}, n \in \mathbb{N}$ is unitary if $Q^{-1} = Q^H$
\item $Q \in \mathbb{R}^{n,n}, n \in \mathbb{N}$ is orthogonal if $Q^{-1} = Q^T \iff ||Qx||_2 = ||\underline{x}||_2 \forall x \in \mathbb{R}^n$
\end{itemize}
The columns and rows form an Orthonormal base of $\mathbb{R}^n$

\subsection{QR-Decomposition}

Our goal here is to answer whether and how it is possible to find orthogonal transformations that convert any given matrix $A \in \mathbb{R}^{m,n}, m \geq n, rank(A) = n$, to upper triangular form, as required for the spplication of the "equivalence transformation idea" to full-rank linear least squares problems.

\underline{\textbf{Gram-Schmidt orthogonalisation:}}
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num35.png}
\end{figure}

From the span property in the above image it follows that:
\begin{center}
$\exists T \in \mathbb{R}^{n,n} \text{ Upper Triangular }: Q = AT$
\end{center}
The inverse of a regular upper triangular matrix is also upper triangular. Hence we have found an upper triangular matrix R defined as:
\begin{center}
$R:= T^{-1} \in \mathbb{R}^{n,n}$
\end{center}
Such that A=QR.
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num36.png}
\end{figure}
We then add m-n zero rows to the bottom of R and complement columns of Q to form an orthonormal basis of $\mathbb{R}^m$, which yields an orthogonal matrix $\tilde{Q} \in \mathbb{R}^{m,m}$
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num37.png}
\end{figure}
From this follows:
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num38.png}
\end{figure}
The economical QR-factorization of $A \in \mathbb{K}^{m,n}, m\geq n$ with Rank(A) = n is unique if we demand $(R_0)_{ii} > 0, i=1,...,n$

\subsection{Computation of QR Decompositions}

Gram-Schmidt is unstable i.e affected by roundoff errors, hence we need better ways to compute the QR Decomposition.

\underline{Idea:} Similar to GE we want to find simple unitary/orthogonal (row) transformations rendering certain matrix elements to zero

\underline{\textbf{Corollary 3.3.3.9}} The product of two orthogonal/unitary matrices of the same size is again orthogonal/unitary.

\underline{\textbf{Householder Reflections:}} A Householder transformation is a linear transformation that describes a reflection about a plane or hyperplane containing the origin. In our case we are using the Householder matrix as orthogonal transformations which step by step convert a matrix into upper triangular form.
\begin{figure}[H]
\centering
\includegraphics[width = 100mm]{Num39.png}
\end{figure}
The householder matrix is given by:
\begin{center}
$H(v) = (I- 2\frac{vv^T}{v^Tv})$
\end{center}
H(v) is orthogonal and symmetric. Special for this case is:
\begin{center}
$\underline{v} = \underline{a} + sgn(a_1)||a||_2\underline{e}_1$ 
\end{center}
We get the following outline for the QR-decomposition using householder matrices:\\
$Q_{n-1}Q_{n-2}....Q_1A = R$ We know the invers of $Q_i$ is $Q_i^H$ 
$\Rightarrow$ we define Q:=$Q_1^H....Q_{n-1}^H$ which is orthogonal (C 3.3.3.9)
$\Rightarrow$ A =QR
HH-reflections can be applied to any matrices. In practice we do not store the Q matrices, but the v vector, because H(v) can be computed from v alone this allows us to be more efficient.\\
The computational cost of QR decomposition using Householder matrices is given by:
\begin{center}
cost(R-factor of A by Householder transformation) = $\displaystyle\sum_{k=1}^{n-1}2(m-k+1)(n-k) = \mathcal{O}(mn^2)$
\end{center}
\underline{\textbf{Householder QR-decomposition in EIGEN:}}
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.32\linewidth}
\includegraphics[width = \linewidth,scale = 1]{Num42.png}
\end{subfigure}
\end{figure}
\underline{\textbf{Givens Rotations}}
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.32\linewidth}
\includegraphics[width = \linewidth,scale = 1]{Num40.png}
\end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}
\includegraphics[width = \linewidth,scale = 1]{Num41.png}
\end{subfigure}
\end{figure}
The advantage of Givens Rotations over Householder reflections is that with Givens we can specifically choose which entries we want to set to zero.
\underline{\textbf{QR-dec of banded matrices:}}
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.32\linewidth}
\includegraphics[width = \linewidth,scale = 1]{Num43.png}
\end{subfigure}
\end{figure}

















\chapter{C++}

\section{Templates and Template Classes in C++} 

\paragraph{Templates:} Let you define the behavior of the class without actually knowing what datatype will be handled by the operations of the class i,e a templated class does not depend on the datatype it deals with. The basic syntax for declaring a templated class:
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.4\linewidth}
\includegraphics[width = \linewidth,scale = 1]{cpp1.png}
\end{subfigure}
\end{figure}
$a\_$type is not a keyword, its an identifier that during the execution of the program will represent a single datatype\\

When defining a function as a member of a templated class, it is necessary to define it as a templated function
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.4\linewidth}
\includegraphics[width = \linewidth,scale = 1]{cpp2.png}
\end{subfigure}
\end{figure}

\underline{\textbf{specialization:}} An instantiated object of a templated class\\
\underline{Example:}
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.3\linewidth}
\includegraphics[width = \linewidth,scale = 1]{cpp3.png}
\end{subfigure}
\begin{subfigure}[b]{0.5\linewidth}
\includegraphics[width = \linewidth,scale = 1]{cpp4.png}
\end{subfigure}
\end{figure}

\paragraph{Keyword: typename} In a template declaration "typename" can be used as an alternative to class to declare type template parameters
\section{Scope Resolution Operator "::"}

The scope Resolution Operator is used for one of the following:

\begin{itemize}
\item Access the global variable, if there is a local variable with the same name
\item Define a function outside the class
\item Access a class's static variable
\item Multiple Inheritance
\item Refer to a class inside another class
\end{itemize}

\section{Namespaces}

Namespaces provide a method for preventing name conflicts in large projects. Symbols declared inside a namespace block are placed in a named scope that prevents them from being mistaken for identically-named symbols in other scopes

\paragraph{using-declaration:} 
\begin{center}
using $ns\_$name::name
\end{center}
Makes the symbol name from the namespace $ns\_name$ accessible for unqualified lookup as if declared in the same class scope, block scope or namespace as where this using-declaration appears

\paragraph{unqualified lookup}

\section{Name Lookup}

The procedure by which a name, when encountered  in a program, is associated with the declaration that introduced


\paragraph{Unqualified name lookup} An unqualified name, is a name that does not appear to the right of a scope resultion operator. Name lookup examines the scopes as described below, until it finds at least one declaration of any kind at which time the lookup stops and no further scopes are examined.\\























\end{document}